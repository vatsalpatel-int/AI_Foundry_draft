# Azure AI Foundry Cost Pipeline

A modular, production-ready pipeline for extracting Azure cost data across multiple AI Foundry projects and storing it in Delta Lake for unified analytics.

---

## ðŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Data Flow](#data-flow)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Configuration](#configuration)
- [Usage](#usage)
- [Data Schema](#data-schema)
- [Scheduling](#scheduling)
- [Troubleshooting](#troubleshooting)

---

## Overview

### Problem Statement

Organizations using Azure AI Foundry often have multiple AI projects across different subscriptions or resource groups. Tracking costs across these projects requires:

- Logging into Azure Portal multiple times
- Manually exporting data from each project
- Consolidating data in spreadsheets
- No historical trend analysis

### Solution

This pipeline automates the entire process:

1. **Authenticates** with Azure using service principal credentials
2. **Extracts** cost data from multiple Azure scopes (subscriptions, resource groups, management groups)
3. **Transforms** data with metadata for lineage tracking
4. **Loads** into Delta Lake with idempotent MERGE operations
5. **Partitions** by date for efficient querying

> **Note**: This pipeline uses the Azure Cost Management **Query API** which supports all subscription types including **Pay-As-You-Go (PAYG)**, Enterprise Agreement (EA), and Microsoft Customer Agreement (MCA).

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              AZURE ORGANIZATION                                  â”‚
â”‚                                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚ AI Project 1 â”‚  â”‚ AI Project 2 â”‚  â”‚ AI Project 3 â”‚  â”‚ AI Project N â”‚        â”‚
â”‚   â”‚  (OpenAI)    â”‚  â”‚  (ML Studio) â”‚  â”‚ (Cognitive)  â”‚  â”‚   (Other)    â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚          â”‚                 â”‚                 â”‚                 â”‚                 â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                     â”‚                                            â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                     â”‚   Azure Cost Management API   â”‚                            â”‚
â”‚                     â”‚   (Aggregates all costs)      â”‚                            â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚ REST API (OAuth 2.0)
                                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          DATABRICKS ENVIRONMENT                                   â”‚
â”‚                                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                         COST PIPELINE                                        â”‚ â”‚
â”‚  â”‚                                                                              â”‚ â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚ â”‚
â”‚  â”‚   â”‚ config   â”‚â”€â”€â”€â–¶â”‚  auth    â”‚â”€â”€â”€â–¶â”‚    data      â”‚â”€â”€â”€â–¶â”‚    delta     â”‚      â”‚ â”‚
â”‚  â”‚   â”‚   .py    â”‚    â”‚   .py    â”‚    â”‚ extractor.py â”‚    â”‚  writer.py   â”‚      â”‚ â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ â”‚
â”‚  â”‚        â”‚               â”‚                 â”‚                   â”‚               â”‚ â”‚
â”‚  â”‚   Load .env      OAuth Token       Query API            MERGE to            â”‚ â”‚
â”‚  â”‚   Validate       Management        JSON â†’ CSV           Delta Lake          â”‚ â”‚
â”‚  â”‚                                                                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                      â”‚                                            â”‚
â”‚                                      â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                           DELTA LAKE (S3/ADLS/DBFS)                          â”‚ â”‚
â”‚  â”‚                                                                               â”‚ â”‚
â”‚  â”‚   ai_foundry_costs/                                                          â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ _delta_log/                    (Transaction log)                       â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ _cost_date=2026-01-05/         (Partitioned data)                      â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ _cost_date=2026-01-06/                                                 â”‚ â”‚
â”‚  â”‚   â””â”€â”€ _cost_date=2026-01-07/                                                 â”‚ â”‚
â”‚  â”‚                                                                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                      â”‚                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚       ANALYTICS LAYER          â”‚
                      â”‚                                â”‚
                      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                      â”‚  â”‚Databricksâ”‚  â”‚ Power BI â”‚   â”‚
                      â”‚  â”‚   SQL    â”‚  â”‚ Tableau  â”‚   â”‚
                      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                      â”‚                                â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Flow

### Step-by-Step Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              DATA FLOW DIAGRAM                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: TRIGGER
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  â€¢ Databricks Job Scheduler (e.g., daily at 6 AM UTC)
  â€¢ Manual execution via CLI or notebook
  â€¢ Backfill mode for historical data
                    â”‚
                    â–¼
STEP 2: CONFIGURATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  config.py                                              â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
  â”‚  â€¢ Load environment variables from .env                 â”‚
  â”‚  â€¢ Validate required credentials                        â”‚
  â”‚  â€¢ Parse multiple Azure scopes                          â”‚
  â”‚  â€¢ Return PipelineConfig object                         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
STEP 3: AUTHENTICATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  auth.py                                                â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€                                                â”‚
  â”‚  â€¢ POST to Azure AD token endpoint                      â”‚
  â”‚  â€¢ Use client credentials (service principal)           â”‚
  â”‚  â€¢ Cache token with expiry tracking                     â”‚
  â”‚  â€¢ Auto-refresh when token expires                      â”‚
  â”‚                                                         â”‚
  â”‚  Token URL: https://login.microsoftonline.com/          â”‚
  â”‚             {tenant}/oauth2/token                       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
STEP 4: QUERY COST DATA (for each scope)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  data_extractor.py                                      â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
  â”‚  â€¢ POST to Cost Management Query API                    â”‚
  â”‚  â€¢ Request: { type: "Usage", timeframe: "Custom",       â”‚
  â”‚               timePeriod: { from, to },                 â”‚
  â”‚               dataset: { granularity: "Daily" } }       â”‚
  â”‚  â€¢ Response: JSON with columns and rows                 â”‚
  â”‚                                                         â”‚
  â”‚  API: https://management.azure.com/{scope}/             â”‚
  â”‚       providers/Microsoft.CostManagement/query          â”‚
  â”‚       ?api-version=2025-03-01                           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
STEP 5: HANDLE PAGINATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  â€¢ Check for nextLink in response                       â”‚
  â”‚  â€¢ If present, GET next page of results                 â”‚
  â”‚  â€¢ Repeat until no more pages                           â”‚
  â”‚  â€¢ Combine all rows into single result                  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
STEP 6: CONVERT TO CSV
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  â€¢ Extract column names from response                   â”‚
  â”‚  â€¢ Convert JSON rows to CSV format                      â”‚
  â”‚  â€¢ Store as bytes for compatibility                     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
STEP 7: TRANSFORM & LOAD
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  delta_writer.py                                        â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
  â”‚  â€¢ Write CSV to temp file                               â”‚
  â”‚  â€¢ Load into Spark DataFrame                            â”‚
  â”‚  â€¢ Add metadata columns:                                â”‚
  â”‚    - _source_scope                                      â”‚
  â”‚    - _source_scope_name                                 â”‚
  â”‚    - _ingestion_timestamp                               â”‚
  â”‚    - _cost_date (partition key)                         â”‚
  â”‚  â€¢ MERGE into Delta table (upsert)                      â”‚
  â”‚  â€¢ Partition by _cost_date                              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
STEP 8: COMPLETE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  â€¢ Log execution summary                                â”‚
  â”‚  â€¢ Report rows written per scope                        â”‚
  â”‚  â€¢ Return stats dictionary                              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Project Structure

```
AI_Foundry/
â”‚
â”œâ”€â”€ ðŸ“„ .env                    # Environment variables (secrets - git ignored)
â”œâ”€â”€ ðŸ“„ .gitignore              # Git ignore rules
â”œâ”€â”€ ðŸ“„ __init__.py             # Package marker
â”œâ”€â”€ ðŸ“„ requirements.txt        # Python dependencies
â”œâ”€â”€ ðŸ“„ README.md               # This documentation
â”‚
â”œâ”€â”€ ðŸ“„ config.py               # Configuration management
â”‚   â”‚   â€¢ Load and validate environment variables
â”‚   â”‚   â€¢ Parse multiple Azure scopes
â”‚   â”‚   â€¢ Return typed configuration objects
â”‚
â”œâ”€â”€ ðŸ“„ auth.py                 # Azure AD authentication
â”‚   â”‚   â€¢ OAuth 2.0 client credentials flow
â”‚   â”‚   â€¢ Token caching and auto-refresh
â”‚   â”‚   â€¢ Authorization header generation
â”‚
â”œâ”€â”€ ðŸ“„ data_extractor.py       # Azure Cost Management Query API client
â”‚   â”‚   â€¢ Query cost data via REST API
â”‚   â”‚   â€¢ Handle pagination (nextLink)
â”‚   â”‚   â€¢ Convert JSON response to CSV
â”‚   â”‚   â€¢ Support multiple scopes
â”‚
â”œâ”€â”€ ðŸ“„ delta_writer.py         # Delta Lake operations
â”‚   â”‚   â€¢ CSV to Spark DataFrame conversion
â”‚   â”‚   â€¢ Metadata column enrichment
â”‚   â”‚   â€¢ Idempotent MERGE operations
â”‚   â”‚   â€¢ Date-based partitioning
â”‚   â”‚   â€¢ Table optimization (OPTIMIZE, VACUUM)
â”‚
â”œâ”€â”€ ðŸ“„ main.py                 # Main orchestrator
â”‚   â”‚   â€¢ CLI argument parsing
â”‚   â”‚   â€¢ Pipeline coordination
â”‚   â”‚   â€¢ Backfill support
â”‚   â”‚   â€¢ Execution reporting
â”‚
â””â”€â”€ ðŸ“ venv/                   # Virtual environment (git ignored)
```

---

## Installation

### Prerequisites

- Python 3.9+
- Databricks Runtime 12.0+ (for production)
- Azure Service Principal with Cost Management Reader role

### Local Setup

```bash
# Clone or navigate to project directory
cd AI_Foundry

# Create virtual environment
python3 -m venv venv

# Activate virtual environment
source venv/bin/activate  # macOS/Linux
# or
.\venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt
```

### Databricks Setup

Upload all `.py` files to Databricks workspace or use Repos to sync from Git.

---

## Configuration

### Environment Variables (.env)

Create a `.env` file with the following configuration:

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REQUIRED: Azure AD Authentication (Service Principal)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Your Azure AD tenant ID (Directory ID)
AZURE_TENANT_ID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx

# Service Principal Application (Client) ID
AZURE_CLIENT_ID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx

# Service Principal Client Secret
AZURE_CLIENT_SECRET=your-client-secret-here

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REQUIRED: Azure Scopes (comma-separated for multiple projects)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# Supported formats:
#   â€¢ Subscription:       subscriptions/{subscription-id}
#   â€¢ Resource Group:     subscriptions/{sub-id}/resourceGroups/{rg-name}
#   â€¢ Management Group:   providers/Microsoft.Management/managementGroups/{mg-id}
#
# Examples:
#   Single scope:
#     AZURE_SCOPES=subscriptions/12345678-1234-1234-1234-123456789012
#
#   Multiple scopes:
#     AZURE_SCOPES=subscriptions/sub-1,subscriptions/sub-2,subscriptions/sub-3
#
#   Mixed scope types:
#     AZURE_SCOPES=subscriptions/sub-1,providers/Microsoft.Management/managementGroups/org-mg

AZURE_SCOPES=subscriptions/your-subscription-id

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REQUIRED: Delta Lake Storage Path
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# Supported storage:
#   â€¢ S3:    s3a://bucket-name/path/to/table
#   â€¢ ADLS:  abfss://container@account.dfs.core.windows.net/path/to/table
#   â€¢ DBFS:  dbfs:/path/to/table

DELTA_TABLE_PATH=s3a://your-bucket/warehouse/ai_foundry_costs

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OPTIONAL: Pipeline Tuning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Legacy settings (kept for backward compatibility, not used by Query API)
POLL_INTERVAL=30
MAX_POLL_ATTEMPTS=60

# API request timeout in seconds (default: 60)
REQUEST_TIMEOUT=60

# Pagination request timeout in seconds (default: 300)
DOWNLOAD_TIMEOUT=300
```

### Azure Service Principal Setup

1. **Create Service Principal** in Azure AD
2. **Assign Role**: `Cost Management Reader` at the appropriate scope
3. **Note down**: Tenant ID, Client ID, Client Secret

```bash
# Azure CLI commands
az ad sp create-for-rbac --name "CostPipelineSP" --role "Cost Management Reader" \
    --scopes /subscriptions/{subscription-id}
```

---

## Usage

### Command Line Interface

```bash
# Activate virtual environment
source venv/bin/activate

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Run for yesterday (default)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python main.py

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Run for a specific date
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python main.py --date 2026-01-05

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Backfill last N days
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python main.py --days 7      # Last 7 days
python main.py --days 30     # Last 30 days

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Use APPEND instead of MERGE (faster, but may create duplicates)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python main.py --no-merge

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Combine options
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python main.py --days 7 --no-merge
```

### Databricks Notebook

```python
# Cell 1: Import and run for yesterday
from main import run_pipeline, run_backfill

result = run_pipeline()
print(f"Rows written: {result['total_rows']}")

# Cell 2: Run for specific date
result = run_pipeline(target_dates=["2026-01-05"])

# Cell 3: Backfill last 7 days
result = run_backfill(days=7)

# Cell 4: Check Delta table stats
from delta_writer import DeltaLakeWriter
writer = DeltaLakeWriter("s3a://bucket/ai_foundry_costs")
stats = writer.get_table_stats()
print(stats)
```

### Scheduling Options

| Schedule | Command / Config | Use Case |
|----------|-----------------|----------|
| **Daily** | `python main.py` | Standard daily cost tracking |
| **Hourly** | Not recommended | Cost data updates ~daily |
| **Weekly backfill** | `python main.py --days 7` | Catch any missed days |
| **Monthly backfill** | `python main.py --days 30` | Historical analysis |
| **Specific date** | `python main.py --date 2026-01-05` | Re-process specific day |

---

## Data Schema

### Raw Data from Azure Cost Management Query API

The Query API returns JSON data which is converted to CSV format. The columns returned depend on the query configuration. With daily granularity, the response includes:

| Column | Data Type | Description |
|--------|-----------|-------------|
| `PreTaxCost` | NUMBER | Cost amount before tax |
| `UsageDate` | NUMBER | Date in YYYYMMDD format |
| `Currency` | STRING | Currency code (e.g., "USD") |

> **Note**: The Query API returns aggregated data. For detailed line-item data with all columns shown below, consider using the Exports API for scheduled exports to blob storage.

### Full Schema Reference (Exports API / Legacy)

The following columns are available when using detailed cost exports:

| Category | Column Name | Data Type | Description |
|----------|-------------|-----------|-------------|
| **Billing** | `BillingAccountId` | STRING | Billing account identifier |
| | `BillingAccountName` | STRING | Billing account name |
| | `BillingPeriodStartDate` | DATE | Start of billing period |
| | `BillingPeriodEndDate` | DATE | End of billing period |
| | `BillingProfileId` | STRING | Billing profile identifier |
| | `BillingProfileName` | STRING | Billing profile name |
| **Invoice** | `InvoiceSectionId` | STRING | Invoice section identifier |
| | `InvoiceSectionName` | STRING | Invoice section name |
| **Product** | `PartNumber` | STRING | Azure part number |
| | `ProductName` | STRING | Full product name |
| **Meter** | `MeterCategory` | STRING | Service category (e.g., "Azure OpenAI Service") |
| | `MeterSubCategory` | STRING | Sub-category (e.g., "GPT-4 Turbo") |
| | `MeterName` | STRING | Specific meter (e.g., "Input Tokens") |
| | `MeterId` | STRING | Unique meter GUID |
| | `MeterRegion` | STRING | Meter region |
| **Resource** | `ResourceLocation` | STRING | Azure region (e.g., "eastus") |
| | `ResourceGroup` | STRING | Resource group name |
| | `ResourceId` | STRING | Full Azure resource ID |
| | `ResourceName` | STRING | Resource display name |
| **Service** | `ServiceName` | STRING | Azure service name |
| | `ServiceTier` | STRING | Service tier |
| **Subscription** | `SubscriptionId` | STRING | Subscription GUID |
| | `SubscriptionName` | STRING | Subscription display name |
| **Cost Center** | `CostCenter` | STRING | Cost center (from tags) |
| **Usage** | `UnitOfMeasure` | STRING | Unit type (e.g., "1K Tokens") |
| | `Quantity` | DOUBLE | Usage quantity |
| | `EffectivePrice` | DOUBLE | Price per unit |
| | `CostInBillingCurrency` | DOUBLE | **Actual cost amount** |
| | `BillingCurrencyCode` | STRING | Currency code (e.g., "USD") |
| **Pricing** | `PricingModel` | STRING | OnDemand, Reservation, Spot |
| | `ChargeType` | STRING | Usage, Purchase, Refund |
| | `Frequency` | STRING | UsageBased, OneTime, Recurring |
| **Publisher** | `PublisherType` | STRING | Microsoft, Marketplace |
| | `PublisherName` | STRING | Publisher name |
| **Reservation** | `ReservationId` | STRING | Reservation ID (if applicable) |
| | `ReservationName` | STRING | Reservation name |
| **Tags** | `Tags` | STRING | JSON string of resource tags |
| **Date** | `Date` | DATE | Cost date |
| **Benefits** | `benefitId` | STRING | Benefit identifier |
| | `benefitName` | STRING | Benefit name |

### Additional Metadata Columns (Added by Pipeline)

| Column | Data Type | Description |
|--------|-----------|-------------|
| `_source_scope` | STRING | Full Azure scope ID |
| `_source_scope_name` | STRING | Human-readable scope name |
| `_ingestion_timestamp` | TIMESTAMP | When data was loaded |
| `_ingestion_date` | DATE | Date of ingestion |
| `_cost_date` | DATE | **Partition key** - the cost date |

### Azure AI Foundry Specific Meters

| MeterCategory | MeterSubCategory | MeterName | UnitOfMeasure |
|---------------|------------------|-----------|---------------|
| Azure OpenAI Service | GPT-4 | Input Tokens | 1K Tokens |
| Azure OpenAI Service | GPT-4 | Output Tokens | 1K Tokens |
| Azure OpenAI Service | GPT-4 Turbo | Input Tokens | 1K Tokens |
| Azure OpenAI Service | GPT-4 Turbo | Output Tokens | 1K Tokens |
| Azure OpenAI Service | GPT-4o | Input Tokens | 1K Tokens |
| Azure OpenAI Service | GPT-4o | Output Tokens | 1K Tokens |
| Azure OpenAI Service | GPT-4o mini | Input Tokens | 1K Tokens |
| Azure OpenAI Service | GPT-4o mini | Output Tokens | 1K Tokens |
| Azure OpenAI Service | Embeddings Ada | Tokens | 1K Tokens |
| Azure OpenAI Service | DALL-E 3 | Standard Images | 1 Image |
| Azure OpenAI Service | DALL-E 3 | HD Images | 1 Image |
| Azure OpenAI Service | Whisper | Audio | 1 Hour |
| Azure OpenAI Service | Text to Speech | Characters | 1M Characters |
| Azure AI Services | Content Safety | Transactions | 1K Transactions |
| Azure Machine Learning | Compute | Various VM SKUs | 1 Hour |

### Example Record

```json
{
  "Date": "2026-01-06",
  "SubscriptionId": "12345678-1234-1234-1234-123456789012",
  "SubscriptionName": "AI Foundry Production",
  "ResourceGroup": "rg-openai-prod",
  "ResourceName": "my-openai-instance",
  "ResourceId": "/subscriptions/.../providers/Microsoft.CognitiveServices/accounts/my-openai-instance",
  "ServiceName": "Azure OpenAI Service",
  "MeterCategory": "Azure OpenAI Service",
  "MeterSubCategory": "GPT-4 Turbo",
  "MeterName": "Output Tokens",
  "MeterId": "abc12345-def6-7890-abcd-ef1234567890",
  "Quantity": 2500.0,
  "UnitOfMeasure": "1K Tokens",
  "EffectivePrice": 0.06,
  "CostInBillingCurrency": 150.00,
  "BillingCurrencyCode": "USD",
  "ChargeType": "Usage",
  "PricingModel": "OnDemand",
  "Tags": "{\"project\":\"customer-chatbot\",\"environment\":\"production\",\"team\":\"ai-platform\"}",
  
  "_source_scope": "subscriptions/12345678-1234-1234-1234-123456789012",
  "_source_scope_name": "subscription-12345678",
  "_ingestion_timestamp": "2026-01-07T06:00:00.000Z",
  "_ingestion_date": "2026-01-07",
  "_cost_date": "2026-01-06"
}
```

---

## Scheduling

### Databricks Workflows

1. **Create a Job** in Databricks Workflows
2. **Task Type**: Python script or Notebook
3. **Schedule**: Daily at 6:00 AM UTC (costs are finalized overnight)
4. **Cluster**: Use a small single-node cluster

```yaml
# Example job configuration
name: AI Foundry Cost Pipeline
schedule:
  quartz_cron_expression: "0 0 6 * * ?"  # Daily at 6 AM UTC
  timezone_id: "UTC"
tasks:
  - task_key: extract_costs
    python_wheel_task:
      package_name: ai_foundry
      entry_point: main
    cluster_spec:
      spark_version: "13.3.x-scala2.12"
      node_type_id: "i3.xlarge"
      num_workers: 0  # Single node
```

---

## Troubleshooting

### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| `ModuleNotFoundError: requests` | venv not activated | Run `source venv/bin/activate` |
| `401 Unauthorized` | Invalid credentials | Check AZURE_CLIENT_ID and AZURE_CLIENT_SECRET |
| `403 Forbidden` | Missing permissions | Assign "Cost Management Reader" role to SP |
| `400 Bad Request` | Invalid scope or date format | Check scope format, ensure dates are valid |
| `Empty response` | No cost data for date range | Verify costs exist for the specified period |
| `spark not defined` | Not in Databricks | Run in Databricks Runtime environment |

### Debugging

```python
# Enable debug logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Test authentication only
from config import load_config
from auth import AzureAuthenticator

config = load_config()
auth = AzureAuthenticator(config.azure)
print(f"Token: {auth.token[:50]}...")  # Print first 50 chars

# Test single scope extraction
from data_extractor import AzureCostExtractor

extractor = AzureCostExtractor(auth)
reports = extractor.extract_costs_for_date(
    scopes=["subscriptions/your-sub-id"],
    target_date="2026-01-06"
)
print(f"Extracted {len(reports)} reports")
```

### Logs Location

- **Databricks**: Driver logs in Spark UI
- **Local**: stdout/stderr or configure file handler

---

## License

Internal use only. Contact your administrator for licensing information.

---

## API Reference

This pipeline uses the **Azure Cost Management Query API**:

- **Endpoint**: `POST https://management.azure.com/{scope}/providers/Microsoft.CostManagement/query?api-version=2025-03-01`
- **Documentation**: [Query - Usage API Reference](https://learn.microsoft.com/en-gb/rest/api/cost-management/query/usage?view=rest-cost-management-2025-03-01)

### Supported Subscription Types

| Subscription Type | Supported |
|-------------------|-----------|
| Pay-As-You-Go (PAYG) | âœ… Yes |
| Enterprise Agreement (EA) | âœ… Yes |
| Microsoft Customer Agreement (MCA) | âœ… Yes |
| CSP (Cloud Solution Provider) | âœ… Yes |

---

## Support

For issues or questions:
1. Check the [Troubleshooting](#troubleshooting) section
2. Review [Azure Cost Management Query API documentation](https://learn.microsoft.com/en-gb/rest/api/cost-management/query/usage)
3. Contact your platform team
test
